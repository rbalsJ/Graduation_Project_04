{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "with open('/content/sorted_eng_data2.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.strip().split(',')\n",
        "    num = int(parts[0].strip().split(':')[1])\n",
        "    origin_lang = parts[1].strip().split(':')[1][1:]\n",
        "    pronun_list = parts[2].strip().split(':')[1][1:-1]\n",
        "\n",
        "    data.append({'num': num, 'origin_lang': origin_lang, 'pronun_list': pronun_list})\n",
        "\n",
        "print(data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0swixdxstZp",
        "outputId": "6fb15fd9-c397-45b9-895d-c4b424cfc148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num': 1, 'origin_lang': '&', 'pronun_list': '앤드'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go9dyBy6jszz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "45347de3-953f-4796-e234-c997506daa61"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-2d993ac0b4e5>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-2d993ac0b4e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, target_seq, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtarget_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_char_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<SOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ast\n",
        "\n",
        "# 데이터 준비\n",
        "data = []\n",
        "\n",
        "with open('/content/sorted_eng_data2.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.strip().split(',')\n",
        "    num = int(parts[0].strip().split(':')[1])\n",
        "    origin_lang = parts[1].strip().split(':')[1][1:]\n",
        "    pronun_list = parts[2].strip().split(':')[1][1:-1]\n",
        "\n",
        "    data.append({'num': num, 'origin_lang': origin_lang, 'pronun_list': pronun_list})\n",
        "\n",
        "# 단어와 발음 사전 생성\n",
        "source_vocab = set()\n",
        "target_vocab = set()\n",
        "for entry in data:\n",
        "    source_vocab.update(entry['origin_lang'])\n",
        "    target_vocab.update(entry['pronun_list'])\n",
        "source_vocab = sorted(list(source_vocab))\n",
        "target_vocab = sorted(list(target_vocab))\n",
        "source_vocab_size = len(source_vocab)\n",
        "target_vocab_size = len(target_vocab)\n",
        "\n",
        "source_char_to_index = {char: index for index, char in enumerate(source_vocab)}\n",
        "target_char_to_index = {char: index for index, char in enumerate(target_vocab)}\n",
        "\n",
        "source_char_to_index['<SOS>'] = len(source_char_to_index)\n",
        "source_char_to_index['<EOS>'] = len(source_char_to_index)\n",
        "target_char_to_index['<SOS>'] = len(target_char_to_index)\n",
        "target_char_to_index['<EOS>'] = len(target_char_to_index)\n",
        "\n",
        "def predict(model, source_word):\n",
        "    model.eval()\n",
        "    source_seq = [source_char_to_index[char] for char in source_word]\n",
        "    source_tensor = torch.tensor(source_seq, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "    decoder_input = torch.tensor([[target_char_to_index['<SOS>']]], device=device)\n",
        "    encoder_outputs, encoder_hidden = model.encoder(model.embedding(source_tensor))\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    predicted_pronun_list = []\n",
        "\n",
        "    for _ in range(10):  # 최대 길이를 임의로 설정. 실제로는 다른 방법으로 결정할 수 있습니다.\n",
        "        decoder_output, decoder_hidden = model.decoder(model.embedding(decoder_input).view(1,1,-1), decoder_hidden)\n",
        "        output = model.output_layer(decoder_output)\n",
        "        _, topi = output.topk(1)\n",
        "        if topi.item() == target_char_to_index['<EOS>']:\n",
        "            break\n",
        "        else:\n",
        "            predicted_pronun_list.append(topi.item())\n",
        "\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "\n",
        "    return ''.join([target_vocab[index] for index in predicted_pronun_list])\n",
        "\n",
        "\n",
        "    return output_string\n",
        "\n",
        "# 모델 정의\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder = nn.GRU(hidden_size, hidden_size)\n",
        "        self.decoder = nn.GRU(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
        "        input_length = input_seq.size(0)\n",
        "        target_length = target_seq.size(0)\n",
        "        batch_size = target_seq.size(1)\n",
        "        target_vocab_size = self.output_layer.out_features\n",
        "\n",
        "        encoder_outputs, encoder_hidden = self.encoder(self.embedding(input_seq))\n",
        "\n",
        "        decoder_input = torch.tensor([[target_char_to_index['<SOS>']] * batch_size], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "        outputs = torch.zeros(target_length, batch_size, target_vocab_size, device=device)\n",
        "\n",
        "        prev_output = decoder_input\n",
        "\n",
        "        for t in range(target_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(self.embedding(prev_output).view(1,1,-1), decoder_hidden)\n",
        "\n",
        "            output = self.output_layer(decoder_output)\n",
        "            outputs[t] = output\n",
        "            if use_teacher_forcing:\n",
        "                prev_output = target_seq[t].view(1)\n",
        "            else:\n",
        "                _, topi = output.topk(1)\n",
        "                prev_output = topi.squeeze().detach()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "hidden_size = 256\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# 모델 초기화\n",
        "model = Seq2Seq(source_vocab_size, hidden_size, target_vocab_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 손실 함수와 최적화 알고리즘 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 훈련\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for entry in data:\n",
        "        source_seq = [source_char_to_index['<SOS>']] + [source_char_to_index[char] for char in entry['origin_lang']] + [source_char_to_index['<EOS>']]\n",
        "        target_seq = [target_char_to_index['<SOS>']] + [target_char_to_index[char] for char in entry['pronun_list']] + [target_char_to_index['<EOS>']]\n",
        "        source_tensor = torch.tensor(source_seq, dtype=torch.long, device=device).view(-1, 1)\n",
        "        target_tensor = torch.tensor(target_seq, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(source_tensor, target_tensor)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        target_tensor = target_tensor[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished!\")\n",
        "\n",
        "test_words = ['anopheles', 'anorak', 'another']\n",
        "for word in test_words:\n",
        "    print(f'Input: {word}, Output: {predict(model, word)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yryGWi6ATN0I",
        "outputId": "9a7c19e7-8fff-4235-ce7e-5a18d54b90de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from jamo import h2j, j2hcj\n",
        "\n",
        "class PronunciationDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data =  pd.read_csv(csv_file).fillna('')\n",
        "        self.jamo_to_idx = {}  # 자모를 인덱스로 매핑\n",
        "        self.origin_lang_to_idx = {}  # 원어 문자를 인덱스로 매핑\n",
        "\n",
        "        self._build_vocab()\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        for word, origin_lang in zip(self.data['word'], self.data['origin_lang']):\n",
        "            self._add_word_to_vocab(word, self.jamo_to_idx)\n",
        "            self._add_origin_lang_to_vocab(origin_lang, self.origin_lang_to_idx)\n",
        "\n",
        "    def _add_word_to_vocab(self, word, vocab):\n",
        "        jamo_chars = list(j2hcj(h2j(word)))  # 초성, 중성, 종성 분리\n",
        "        for char in jamo_chars:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = len(vocab)\n",
        "\n",
        "    def _add_origin_lang_to_vocab(self, origin_lang, vocab):\n",
        "        for char in origin_lang:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = len(vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = self.data.iloc[idx]['word']\n",
        "        origin_lang = self.data.iloc[idx]['origin_lang']\n",
        "        return self._encode_word(word, self.jamo_to_idx), self._encode_origin_lang(origin_lang, self.origin_lang_to_idx)\n",
        "\n",
        "    def _encode_word(self, word, vocab):\n",
        "        jamo_chars = list(j2hcj(h2j(word)))  # 초성, 중성, 종성 분리\n",
        "        return [vocab[char] for char in jamo_chars]\n",
        "\n",
        "    def _encode_origin_lang(self, origin_lang, vocab):\n",
        "        return [vocab[char] for char in origin_lang]\n",
        "\n",
        "# 데이터 로딩\n",
        "dataset = PronunciationDataset('/content/our_sam_db.txt')\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "SJo2eQHbLoSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_size = len(dataset.origin_lang_to_idx)  # 원어 문자 집합 크기\n",
        "output_size = 54  # 자모 집합 크기\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 3\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 임베딩 계층 추가\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        self.encoder = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)  # 수정된 부분\n",
        "        self.decoder = nn.LSTM(hidden_size * 2 if output_size > hidden_size else hidden_size,\n",
        "                               hidden_size,\n",
        "                               num_layers,\n",
        "                               batch_first=True)\n",
        "\n",
        "        # Linear layer's input size is adjusted according to the size of decoder's input.\n",
        "        self.fc = nn.Linear(hidden_size * 2 if output_size > hidden_size else hidden_size,\n",
        "                            output_size)\n",
        "\n",
        "    def forward(self, x, initial_hidden=None, initial_cell=None):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Encoder\n",
        "        _, (hidden, cell) = self.encoder(x)\n",
        "\n",
        "        # Use provided initial_hidden and initial_cell if available\n",
        "        if initial_hidden is not None and initial_cell is not None:\n",
        "            hidden = initial_hidden\n",
        "            cell = initial_cell\n",
        "\n",
        "        # Decoder의 초기 은닉 상태와 셀 상태를 설정\n",
        "        decoder_input = torch.zeros(x.size(0), 1,self.hidden_size).to(x.device)\n",
        "\n",
        "        outputs=[]\n",
        "\n",
        "        for _ in range(x.size(1)):\n",
        "            output,(hidden, cell)=self.decoder(decoder_input,(hidden,cell))\n",
        "            output=self.fc(output)\n",
        "            outputs.append(output)\n",
        "\n",
        "            # Get the top k values and their indices\n",
        "            topv, topi = output.topk(1)\n",
        "\n",
        "            # Convert indices to embedded vectors\n",
        "            decoder_input = self.embedding(topi.squeeze().detach()).unsqueeze(1)\n",
        "\n",
        "        return torch.cat(outputs,dim=1)\n",
        "\n",
        "# 모델 초기화\n",
        "model = Seq2Seq(input_size, hidden_size, output_size, num_layers)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 원어 문자와 자모의 최대 길이를 구합니다\n",
        "max_source_length = max([len(source) for source, _ in dataset])\n",
        "max_target_length = max([len(target) for _, target in dataset])\n",
        "\n",
        "source_pad = input_size - 1\n",
        "target_pad = output_size - 1\n",
        "\n",
        "# 데이터를 패딩하여 하나의 텐서로 만듭니다\n",
        "padded_sources = []\n",
        "padded_targets = []\n",
        "for source, target in dataset:\n",
        "    padded_source = source + [source_pad] * (max_source_length - len(source))\n",
        "    padded_target = target + [target_pad] * (max_target_length - len(target))\n",
        "    padded_sources.append(padded_source)\n",
        "    padded_targets.append(padded_target)\n",
        "\n",
        "# 텐서로 변환\n",
        "tensor_sources = torch.tensor(padded_sources, dtype=torch.long)  # dtype을 명시적으로 지정\n",
        "tensor_targets = torch.tensor(padded_targets, dtype=torch.long)\n",
        "\n",
        "# 데이터셋 분할 전에 텐서 변환 및 패딩\n",
        "tensor_dataset = TensorDataset(tensor_sources, tensor_targets)\n",
        "\n",
        "# 데이터셋 크기 계산\n",
        "dataset_size = len(tensor_dataset)\n",
        "train_size = int(0.75 * dataset_size)\n",
        "validation_size = int(0.15 * dataset_size)\n",
        "test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "# 텐서 형태의 데이터셋으로 분할\n",
        "train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
        "                                                               [train_size,\n",
        "                                                                validation_size,\n",
        "                                                                test_size])\n",
        "\n",
        "# 각 데이터셋에 대해 DataLoader 생성\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for batch_source, batch_target in train_dataloader:\n",
        "        batch_source = batch_source.to(device)\n",
        "        batch_target = batch_target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_source)\n",
        "        loss = criterion(outputs.view(-1, output_size), batch_target.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_source, batch_target in test_dataloader:\n",
        "            batch_source = batch_source.to(device)\n",
        "            batch_target = batch_target.to(device)\n",
        "\n",
        "            outputs = model(batch_source)\n",
        "            predicted = torch.argmax(outputs, dim=2)\n",
        "\n",
        "            total_predictions += batch_target.size(0) * batch_target.size(1)\n",
        "            correct_predictions += (predicted == batch_target).sum().item()\n",
        "\n",
        "\n",
        "    # Convert indices back to original words\n",
        "    idx_to_origin_lang = {idx: char for char, idx in dataset.origin_lang_to_idx.items()}\n",
        "    all_original_words = []\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_dataloader):.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print('Training finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "qffCZjbp-YIy",
        "outputId": "91967c32-62d0-40ef-9fb8-30ce39c0a72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-777d686f606c>\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'view'"
          ]
        }
      ]
    }
  ]
}