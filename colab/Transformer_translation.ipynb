{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c230c1045ed4e5f9236e53c5cbdf3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b37a5e0a47944df8a45d01860c1cc571",
              "IPY_MODEL_d25200e0c8e44cc48f69ed6206d9016c",
              "IPY_MODEL_39f8d2300fd246ed8a1461065a7b5b09"
            ],
            "layout": "IPY_MODEL_07b8f9da03194cdfac621ae9ae859ab1"
          }
        },
        "b37a5e0a47944df8a45d01860c1cc571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ce4c49f9804e2daee824fbbfd3270c",
            "placeholder": "​",
            "style": "IPY_MODEL_5902fea5ff0f46eda2fa25adbe00a731",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "d25200e0c8e44cc48f69ed6206d9016c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6db5a307a42941e889d0aac9d09a662a",
            "max": 59103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb88c2acaa3246bf82896d54e515ada0",
            "value": 59103
          }
        },
        "39f8d2300fd246ed8a1461065a7b5b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7525c7ae5d794239a1670bcb68a19a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_0470b11d84d6456c8290714f394439b1",
            "value": " 59.1k/59.1k [00:00&lt;00:00, 298kB/s]"
          }
        },
        "07b8f9da03194cdfac621ae9ae859ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ce4c49f9804e2daee824fbbfd3270c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5902fea5ff0f46eda2fa25adbe00a731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6db5a307a42941e889d0aac9d09a662a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb88c2acaa3246bf82896d54e515ada0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7525c7ae5d794239a1670bcb68a19a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0470b11d84d6456c8290714f394439b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a422c17bbca34aa5a288713b7cc3fc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fa6af83a77f4f1b9505d8363bcc2bf8",
              "IPY_MODEL_f0aa164b04d5407bbe3209e1670071f8",
              "IPY_MODEL_3275f2cf6c2042bc90d1ad072eca285c"
            ],
            "layout": "IPY_MODEL_f8b3d9f7a0da41d8b09e50a2bca53861"
          }
        },
        "0fa6af83a77f4f1b9505d8363bcc2bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea70492c835243da9775d4953407a336",
            "placeholder": "​",
            "style": "IPY_MODEL_0c675add6c384bfd8cc585f34b0970e7",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "f0aa164b04d5407bbe3209e1670071f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae47e30e807b4397b6ba58c3e4db5f1b",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f96f1c560e8640c588beb355d524d358",
            "value": 65
          }
        },
        "3275f2cf6c2042bc90d1ad072eca285c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b9841198cda400099895688df597823",
            "placeholder": "​",
            "style": "IPY_MODEL_7ecc04878e0640d09f436652dd1794c7",
            "value": " 65.0/65.0 [00:00&lt;00:00, 2.62kB/s]"
          }
        },
        "f8b3d9f7a0da41d8b09e50a2bca53861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea70492c835243da9775d4953407a336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c675add6c384bfd8cc585f34b0970e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae47e30e807b4397b6ba58c3e4db5f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96f1c560e8640c588beb355d524d358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b9841198cda400099895688df597823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ecc04878e0640d09f436652dd1794c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99a53822b10841a2b585204ccf5bd1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8f56958d3414de2b20fec51c8ac1d22",
              "IPY_MODEL_b6628775b3854f509c9479debf0f70b5",
              "IPY_MODEL_2929cb1d5c53445a96bca572259fcbef"
            ],
            "layout": "IPY_MODEL_ab72d0ce54d541f7a2bdf9d11a97f9bc"
          }
        },
        "c8f56958d3414de2b20fec51c8ac1d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839ca4e82cce4a9f83f892c47e4271f3",
            "placeholder": "​",
            "style": "IPY_MODEL_39a90263d94542cf8379806852e3a4b8",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b6628775b3854f509c9479debf0f70b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb4b4fbe732c473d8a4333c7f6eb52c9",
            "max": 487,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53daf758f5da42899a3c765ab665d6f4",
            "value": 487
          }
        },
        "2929cb1d5c53445a96bca572259fcbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6f08e6550c4cdc9641f2170e524b26",
            "placeholder": "​",
            "style": "IPY_MODEL_c58f35170f444dd8aaf284f793bc9951",
            "value": " 487/487 [00:00&lt;00:00, 19.7kB/s]"
          }
        },
        "ab72d0ce54d541f7a2bdf9d11a97f9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839ca4e82cce4a9f83f892c47e4271f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a90263d94542cf8379806852e3a4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb4b4fbe732c473d8a4333c7f6eb52c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53daf758f5da42899a3c765ab665d6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a6f08e6550c4cdc9641f2170e524b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c58f35170f444dd8aaf284f793bc9951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/monologg/KoCharELECTRA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MihotM77vuss",
        "outputId": "5f408182-1475-41fc-b309-0e7cfba1cabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n",
            "Cloning into 'KoCharELECTRA'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 37 (delta 16), reused 22 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (37/37), 58.87 KiB | 242.00 KiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxJmTZJu4b3",
        "outputId": "338b02fc-28f0-4942-cc39-70d88a57ce4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import re\n",
        "pickle_file_path = \"/content/drive/MyDrive/eng_database.pkl\"\n",
        "with open(pickle_file_path,'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "data_alp = [\n",
        "            ('에이', '외래어', '어휘', None, '에이', None, '일반어', '명사', 'A', None),\n",
        "            ('에이', '외래어', '어휘', None, '에이', None, '일반어', '명사', 'a', None),\n",
        "            ('비', '외래어', '어휘', None, '비', None, '일반어', '명사', 'B', None),\n",
        "            ('비', '외래어', '어휘', None, '비', None, '일반어', '명사', 'b', None),\n",
        "            ('시', '외래어', '어휘', None, '시', None, '일반어', '명사', 'C', None),\n",
        "            ('시', '외래어', '어휘', None, '시', None, '일반어', '명사', 'c', None),\n",
        "            ('디', '외래어', '어휘', None, '디', None, '일반어', '명사', 'D', None),\n",
        "            ('디', '외래어', '어휘', None, '디', None, '일반어', '명사', 'd', None),\n",
        "            ('이', '외래어', '어휘', None, '이', None, '일반어', '명사', 'E', None),\n",
        "            ('이', '외래어', '어휘', None, '이', None, '일반어', '명사', 'e', None),\n",
        "            ('에프', '외래어', '어휘', None, '에프', None, '일반어', '명사', 'F', None),\n",
        "            ('에프', '외래어', '어휘', None, '에프', None, '일반어', '명사', 'f', None),\n",
        "            ('지', '외래어', '어휘', None, '지', None, '일반어', '명사', 'G', None),\n",
        "            ('지', '외래어', '어휘', None, '지', None, '일반어', '명사', 'g', None),\n",
        "            ('에이치', '외래어', '어휘', None, '에이치', None, '일반어', '명사', 'H', None),\n",
        "            ('에이치', '외래어', '어휘', None, '에이치', None, '일반어', '명사', 'h', None),\n",
        "            ('아이', '외래어', '어휘', None, '아이', None, '일반어', '명사', 'I', None),\n",
        "            ('아이', '외래어', '어휘', None, '아이', None, '일반어', '명사', 'i', None),\n",
        "            ('제이', '외래어', '어휘', None, '제이', None, '일반어', '명사', 'J', None),\n",
        "            ('제이', '외래어', '어휘', None, '제이', None, '일반어', '명사', 'j', None),\n",
        "            ('케이', '외래어', '어휘', None, '케이', None, '일반어', '명사', 'K', None),\n",
        "            ('케이', '외래어', '어휘', None, '케이', None, '일반어', '명사', 'k', None),\n",
        "            ('엘', '외래어', '어휘', None, '엘', None, '일반어', '명사', 'L', None),\n",
        "            ('엘', '외래어', '어휘', None, '엘', None, '일반어', '명사', 'l', None),\n",
        "            ('엠', '외래어', '어휘', None, '엠', None, '일반어', '명사', 'M', None),\n",
        "            ('엠', '외래어', '어휘', None, '엠', None, '일반어', '명사', 'm', None),\n",
        "            ('엔', '외래어', '어휘', None, '엔', None, '일반어', '명사', 'N', None),\n",
        "            ('엔', '외래어', '어휘', None, '엔', None, '일반어', '명사', 'n', None),\n",
        "            ('오', '외래어', '어휘', None, '오', None, '일반어', '명사', 'O', None),\n",
        "            ('오', '외래어', '어휘', None, '오', None, '일반어', '명사', 'o', None),\n",
        "            ('피', '외래어', '어휘', None, '피', None, '일반어', '명사', 'P', None),\n",
        "            ('피', '외래어', '어휘', None, '피', None, '일반어', '명사', 'p', None),\n",
        "            ('큐', '외래어', '어휘', None, '큐', None, '일반어', '명사', 'Q', None),\n",
        "            ('큐', '외래어', '어휘', None, '큐', None, '일반어', '명사', 'q', None),\n",
        "            ('알', '외래어', '어휘', None, '알', None, '일반어', '명사', 'R', None),\n",
        "            ('알', '외래어', '어휘', None, '알', None, '일반어', '명사', 'r', None),\n",
        "            ('에스', '외래어', '어휘', None, '에스', None, '일반어', '명사', 'S', None),\n",
        "            ('에스', '외래어', '어휘', None, '에스', None, '일반어', '명사', 's', None),\n",
        "            ('티', '외래어', '어휘', None, '티', None, '일반어', '명사', 'T', None),\n",
        "            ('티', '외래어', '어휘', None, '티', None, '일반어', '명사', 't', None),\n",
        "            ('유', '외래어', '어휘', None, '유', None, '일반어', '명사', 'U', None),\n",
        "            ('유', '외래어', '어휘', None, '유', None, '일반어', '명사', 'u', None),\n",
        "            ('브이', '외래어', '어휘', None, '브이', None, '일반어', '명사', 'V', None),\n",
        "            ('브이', '외래어', '어휘', None, '브이', None, '일반어', '명사', 'v', None),\n",
        "            ('더블유', '외래어', '어휘', None, '더블유', None, '일반어', '명사', 'W', None),\n",
        "            ('더블유', '외래어', '어휘', None, '더블유', None, '일반어', '명사', 'w', None),\n",
        "            ('엑스', '외래어', '어휘', None, '엑스', None, '일반어', '명사', 'X', None),\n",
        "            ('엑스', '외래어', '어휘', None, '엑스', None, '일반어', '명사', 'x', None),\n",
        "            ('와이', '외래어', '어휘', None, '와이', None, '일반어', '명사', 'Y', None),\n",
        "            ('와이', '외래어', '어휘', None, '와이', None, '일반어', '명사', 'y', None),\n",
        "            ('지', '외래어', '어휘', None, '지', None, '일반어', '명사', 'Z', None),\n",
        "            ('지', '외래어', '어휘', None, '지', None, '일반어', '명사', 'z', None),\n",
        "        ]\n",
        "data_ = set()\n",
        "\n",
        "for idx, val in enumerate(data):\n",
        "  data_.add(val['origin_lang'])\n",
        "\n",
        "for idx, val in enumerate(data_alp):\n",
        "  if val[8] not in data_:\n",
        "    data.append({'origin_lang': val[8], 'pronun_list': val[0]})\n",
        "\n",
        "for idx, val in enumerate(data):\n",
        "  data[idx]['origin_lang'] = re.sub('[^a-zA-Z]', '', val['origin_lang']).strip()\n"
      ],
      "metadata": {
        "id": "oiFu_5ETvyRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "num_data = pd.DataFrame(data)\n",
        "\n",
        "print(num_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JYzkY01v_KP",
        "outputId": "b106075f-0f8e-45d6-9aa8-461b537522e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     origin_lang pronun_list\n",
            "0      gargoyles        가고일스\n",
            "1         gargle          가글\n",
            "2        gahnite        가나이트\n",
            "3     ganaxolone        가낙솔론\n",
            "4     garnetting         가네팅\n",
            "..           ...         ...\n",
            "95     gasengine        가스엔진\n",
            "96       gasoven        가스오븐\n",
            "97   gasexpander      가스익스팬더\n",
            "98     gascarbon        가스카본\n",
            "99  gascollector       가스컬렉터\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "G2JqPcuIycKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "from torch.nn import ModuleList\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "ModuleList는 목록에 하위 모듈을 보관하것\n",
        "이때 모듈들은 파이썬 리스트들 처럼 인덱스를 사용할 수 있다.\n",
        "\"\"\"\n",
        "def clones(module, N):\n",
        "  return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\"\"\"\n",
        "디코더에서 어텐션 스코어 매트릭스에서\n",
        "이후의 값들에 대해 -∞으로 마스킹 처리해주기 위한 함수\n",
        "(1, size, size)의 마스크를 리턴한다.\n",
        "\"\"\"\n",
        "def subsequent_mask(size):\n",
        "  \"Mask out subsequent positions.\"\n",
        "  attn_shape = (1, size, size)\n",
        "  subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "  return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "def log(t, eps=1e-9):\n",
        "    return torch.log(t + eps)\n",
        "\n",
        "def gumbel_noise(t):\n",
        "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
        "    return -log(-log(noise))\n",
        "\n",
        "def gumbel_sample(t, temperature = 1.):\n",
        "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
        "\n",
        "def prob_mask_like(t, prob):\n",
        "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
        "\n",
        "def mask_with_tokens(t, token_ids):\n",
        "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
        "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
        "    return mask\n",
        "\n",
        "def get_mask_subset_with_prob(mask, prob):\n",
        "    batch, seq_len, device = *mask.shape, mask.device\n",
        "    max_masked = math.ceil(prob * seq_len)\n",
        "\n",
        "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
        "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
        "    mask_excess = mask_excess[:, :max_masked]\n",
        "\n",
        "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
        "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
        "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
        "\n",
        "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
        "    new_mask.scatter_(-1, sampled_indices, 1)\n",
        "    return new_mask[:, 1:].bool()\n",
        "\n",
        "def temperature_sampling(logits, temperature):\n",
        "  if temperature is None or temperature == 0.0:\n",
        "    return torch.argmax(logits)\n",
        "  probs = F.softmax(logits / temperature)\n",
        "  pred_ids = probs.cpu().multinomial(probs.size()[1], replacement=False)\n",
        "  return pred_ids"
      ],
      "metadata": {
        "id": "mBQIdj-oyp6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.activations import get_activation\n",
        "\n",
        "\"\"\"\n",
        "self-Attention의 경우 Query Q, Key K, Value V를 입력으로 받아\n",
        "MatMul(Q,K) -> Scale -> Masking(opt. Decoder) -> Softmax -> MatMul(result, V)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def self_attention(query, key, value, mask=None):\n",
        "  key_transpose = torch.transpose(key,-2,-1)                      # (bath, head_num, d_k, token_)\n",
        "  matmul_result = torch.matmul(query,key_transpose)                # MatMul(Q,K)\n",
        "  d_k = query.size()[-1]\n",
        "  attention_score = matmul_result/math.sqrt(d_k)                  # Scale\n",
        "\n",
        "  if mask is not None:\n",
        "    attention_score = attention_score.masked_fill(mask == 0, -1e20)\n",
        "\n",
        "  softmax_attention_score = F.softmax(attention_score,dim=-1)  # 어텐션 값\n",
        "  result = torch.matmul(softmax_attention_score,value)\n",
        "\n",
        "  return result, softmax_attention_score\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "멀티헤드 어텐션\n",
        "MultiHead(Q,K,V) = Concat(head_1,head_2,...head_n)W^O\n",
        "head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
        "W^Q는 모델의 dimension x d_k\n",
        "W^K는 모델의 dimension x d_k\n",
        "W^V는 모델의 dimension x d_v\n",
        "W^O는 d_v * head 갯수 x 모델 dimension\n",
        "논문에서는 헤더의 갯수를 8개 사용\n",
        "\"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, head_num =8 , d_model = 512,dropout = 0.1):\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "\n",
        "    # print(d_model % head_num)\n",
        "    # assert d_model % head_num != 0 # d_model % head_num == 0 이 아닌경우 에러메세지 발생\n",
        "\n",
        "    self.head_num = head_num\n",
        "    self.d_model = d_model\n",
        "    self.d_k = self.d_v = d_model // head_num\n",
        "\n",
        "    self.w_q = nn.Linear(d_model,d_model)\n",
        "    self.w_k = nn.Linear(d_model,d_model)\n",
        "    self.w_v = nn.Linear(d_model,d_model)\n",
        "    self.w_o = nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.self_attention = self_attention\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    if mask is not None:\n",
        "      # Same mask applied to all h heads.\n",
        "      mask = mask.unsqueeze(1)\n",
        "\n",
        "    batche_num = query.size(0)\n",
        "\n",
        "    query = self.w_q(query).view(batche_num, -1, self.head_num, self.d_k).transpose(1, 2)\n",
        "    key = self.w_k(key).view(batche_num, -1, self.head_num, self.d_k).transpose(1, 2)\n",
        "    value = self.w_v(value).view(batche_num, -1, self.head_num, self.d_k).transpose(1, 2)\n",
        "\n",
        "    attention_result, attention_score = self.self_attention(query, key, value, mask)\n",
        "\n",
        "    # 원래의 모양으로 다시 변형해준다.\n",
        "    # torch.continuos는 다음행과 열로 이동하기 위한 stride가 변형되어\n",
        "    # 메모리 연속적으로 바꿔야 한다!\n",
        "    # 참고 문서: https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
        "    attention_result = attention_result.transpose(1,2).contiguous().view(batche_num, -1, self.head_num * self.d_k)\n",
        "\n",
        "\n",
        "    return self.w_o(attention_result)\n",
        "\n",
        "\"\"\"\n",
        "Position-wise Feed-Forward Networks\n",
        "FFN(x) = max(0,xW_1 + b_1)W_2+b2\n",
        "입력과 출력은 모두 d_model의 dimension을 가지고\n",
        "내부의 레이어는 d_model * 4의 dimension을 가진다.\n",
        "\"\"\"\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model, dropout = 0.1):\n",
        "    super(FeedForward,self).__init__()\n",
        "    self.w_1 = nn.Linear(d_model, d_model*4)\n",
        "    self.w_2 = nn.Linear(d_model*4, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\"\"\"\n",
        "Layer Normalization\n",
        ": layer의 hidden unit들에 대해서 mean과 variance를 구한다.\n",
        "nn.Parameter는 모듈 파라미터로 여겨지는 텐서\n",
        "\"\"\"\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm,self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim =True) # 평균\n",
        "    std = x.std(-1, keepdim=True)    # 표준편차\n",
        "\n",
        "    return self.a_2 * (x-mean)/ (std + self.eps) + self.b_2\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, size, dropout):\n",
        "    super(ResidualConnection,self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout((sublayer(self.norm(x))))\n",
        "\n",
        "\"\"\"\n",
        "Encoder 블록은 FeedForward 레이어와 MultiHead 어텐션 레이어를 가진다.\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, head_num, dropout):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.multi_head_attention = MultiHeadAttention(d_model= d_model, head_num= head_num)\n",
        "    self.residual_1 = ResidualConnection(d_model,dropout=dropout)\n",
        "\n",
        "    self.feed_forward = FeedForward(d_model)\n",
        "    self.residual_2 = ResidualConnection(d_model,dropout=dropout)\n",
        "\n",
        "  def forward(self, input, mask):\n",
        "    x = self.residual_1(input, lambda x: self.multi_head_attention(x, x, x, mask))\n",
        "    x = self.residual_2(x, lambda x: self.feed_forward(x))\n",
        "    return x\n",
        "\n",
        "\"\"\"\n",
        "Decoder 블록은 FeedForward 레이어와 MultiHead 어텐션, Masked Multihead 어텐션 레이어를 가진다.\n",
        "MaskedMultiHeadAttention -> MultiHeadAttention(encoder-decoder attention) -> FeedForward\n",
        "\"\"\"\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model,head_num, dropout):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.masked_multi_head_attention = MultiHeadAttention(d_model= d_model, head_num= head_num)\n",
        "    self.residual_1 = ResidualConnection(d_model,dropout=dropout)\n",
        "\n",
        "    self.encoder_decoder_attention = MultiHeadAttention(d_model= d_model, head_num= head_num)\n",
        "    self.residual_2 = ResidualConnection(d_model,dropout=dropout)\n",
        "\n",
        "    self.feed_forward= FeedForward(d_model)\n",
        "    self.residual_3 = ResidualConnection(d_model,dropout=dropout)\n",
        "\n",
        "\n",
        "  def forward(self, target, encoder_output, target_mask, encoder_mask):\n",
        "    # target, x, target_mask, input_mask\n",
        "    x = self.residual_1(target, lambda x: self.masked_multi_head_attention(x, x, x, target_mask))\n",
        "    x = self.residual_2(x, lambda x: self.encoder_decoder_attention(x, encoder_output, encoder_output, encoder_mask))\n",
        "    x = self.residual_3(x, self.feed_forward)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_num, d_model):\n",
        "    super(Embeddings,self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_num,d_model)\n",
        "    self.d_model = d_model\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    1) 임베딩 값에 math.sqrt(self.d_model)을 곱해주는 이유는 무엇인지 찾아볼것\n",
        "    2) nn.Embedding에 다시 한번 찾아볼것\n",
        "    \"\"\"\n",
        "    return self.emb(x) * math.sqrt(self.d_model)\n",
        "\"\"\"\n",
        "Positional Encoding\n",
        "트랜스포머는 RNN이나 CNN을 사용하지 않기 때문에 입력에 순서 값을 반영해줘야 한다.\n",
        "예) 나는 어제의 오늘\n",
        "PE (pos,2i) = sin(pos/10000^(2i/d_model))\n",
        "PE (pos,2i+1) = cos(pos/10000^(2i/d_model))\n",
        "\"\"\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, max_seq_len, d_model,dropout=0.1):\n",
        "    super(PositionalEncoding,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "    position = torch.arange(0,max_seq_len).unsqueeze(1)\n",
        "    base = torch.ones(d_model//2).fill_(10000)\n",
        "    pow_term = torch.arange(0, d_model, 2) / torch.tensor(d_model,dtype=torch.float32)\n",
        "    div_term = torch.pow(base,pow_term)\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position / div_term)\n",
        "    pe[:, 1::2] = torch.cos(position / div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    # pe를 학습되지 않는 변수로 등록\n",
        "    self.register_buffer('positional_encoding', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + Variable(self.positional_encoding[:, :x.size(1)], requires_grad=False)\n",
        "    return self.dropout(x)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, dim, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    t = torch.arange(x.shape[1], device=x.device)\n",
        "    return self.embedding(t)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, d_model, vocab_num):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj_1 = nn.Linear(d_model, d_model*4)\n",
        "    self.proj_2 = nn.Linear(d_model*4, vocab_num)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.proj_1(x)\n",
        "    x = self.proj_2(x)\n",
        "    return x\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,vocab_num, d_model, max_seq_len, head_num, dropout, N):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.embedding = Embeddings(vocab_num, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(max_seq_len,d_model)\n",
        "\n",
        "    self.encoders = clones(Encoder(d_model=d_model, head_num=head_num, dropout=dropout), N)\n",
        "    self.decoders = clones(Decoder(d_model=d_model, head_num=head_num, dropout=dropout), N)\n",
        "\n",
        "    self.generator = Generator(d_model, vocab_num)\n",
        "\n",
        "  def forward(self, input, target, input_mask, target_mask, labels=None):\n",
        "      x = self.positional_encoding(self.embedding(input))\n",
        "      for encoder in self.encoders:\n",
        "        x = encoder(x, input_mask)\n",
        "\n",
        "      target = self.positional_encoding(self.embedding(target))\n",
        "      for decoder in self.decoders:\n",
        "        # target, encoder_output, target_mask, encoder_mask)\n",
        "        target = decoder(target, x, target_mask, input_mask)\n",
        "\n",
        "      lm_logits = self.generator(target)\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "        # Shift so that tokens < n predict n\n",
        "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        # Flatten the tokens\n",
        "        loss_fct = CrossEntropyLoss(ignore_index=0)\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "      return lm_logits, loss\n",
        "  def encode(self,input, input_mask):\n",
        "    x = self.positional_encoding(self.embedding(input))\n",
        "    for encoder in self.encoders:\n",
        "      x = encoder(x, input_mask)\n",
        "    return x\n",
        "\n",
        "  def decode(self, encode_output, encoder_mask, target, target_mask):\n",
        "    target = self.positional_encoding(self.embedding(target))\n",
        "    for decoder in self.decoders:\n",
        "      #target, encoder_output, target_mask, encoder_mask\n",
        "      target = decoder(target, encode_output, target_mask, encoder_mask)\n",
        "\n",
        "    lm_logits = self.generator(target)\n",
        "\n",
        "    return lm_logits\n",
        "class TransformerMRCHead(nn.Module):\n",
        "  def __init__(self, dim, num_labels,hidden_dropout_prob=0.3):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(dim, 1*dim)\n",
        "    self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "    self.out_proj = nn.Linear(1*dim,num_labels)\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    # x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x = get_activation(\"gelu\")(x)  # although BERT uses tanh here, it seems Electra authors used gelu here\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "    return x\n",
        "\n",
        "class TransformerMRCModel(nn.Module):\n",
        "  def __init__(self, vocab_size, dim, depth, max_seq_len, head_num, num_labels=2, causal=False, dropout_prob=0.2):\n",
        "    super().__init__()\n",
        "    self.transformer = TransformerLM(\n",
        "      vocab_size=vocab_size,\n",
        "      dim=dim,\n",
        "      depth=depth,\n",
        "      max_seq_len=max_seq_len,\n",
        "      head_num=head_num,\n",
        "    )\n",
        "    self.mrc_head = TransformerMRCHead(dim, num_labels)\n",
        "\n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              input_mask=None,\n",
        "              start_positions=None,\n",
        "              end_positions=None,\n",
        "              **kwargs):\n",
        "    # 1. transformer의 출력\n",
        "    _, outputs = self.transformer(input_ids, input_mask)\n",
        "\n",
        "    # 2. mrc를 위한\n",
        "    logits = self.mrc_head(outputs)\n",
        "\n",
        "    start_logits, end_logits = logits.split(1, dim=-1)\n",
        "    start_logits = start_logits.squeeze(-1)\n",
        "    end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "    if start_positions is not None and end_positions is not None:\n",
        "      # If we are on multi-GPU, split add a dimension\n",
        "      if len(start_positions.size()) > 1:\n",
        "        start_positions = start_positions.squeeze(-1)\n",
        "      if len(end_positions.size()) > 1:\n",
        "        end_positions = end_positions.squeeze(-1)\n",
        "      # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "      ignored_index = start_logits.size(1)\n",
        "      start_positions.clamp_(0, ignored_index)\n",
        "      end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "      loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "      start_loss = loss_fct(start_logits, start_positions)\n",
        "      end_loss = loss_fct(end_logits, end_positions)\n",
        "      total_loss = (start_loss + end_loss) / 2\n",
        "      return total_loss\n",
        "    else:\n",
        "      return start_logits, end_logits\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "  def __init__(self, vocab_size, dim=512,  depth= 12, max_seq_len=512, head_num=8, dropout= 0.1):\n",
        "    super(TransformerLM,self).__init__()\n",
        "\n",
        "    self.token_emb= nn.Embedding(vocab_size, dim)\n",
        "    self.position_emb = PositionalEmbedding(dim,max_seq_len)\n",
        "    self.encoders = clones(Encoder(d_model=dim, head_num=head_num, dropout=dropout), depth)\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "    self.lm_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.Linear(dim, vocab_size)\n",
        "            )\n",
        "\n",
        "  def forward(self, input_ids, input_mask):\n",
        "    x = self.token_emb(input_ids)\n",
        "    x = x + self.position_emb(input_ids).type_as(x)\n",
        "\n",
        "    for encoder in self.encoders:\n",
        "      x = encoder(x, input_mask)\n",
        "    x = self.norm(x)\n",
        "\n",
        "    return self.lm_head(x), x  # lm_head, performer_embedding\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  pass"
      ],
      "metadata": {
        "id": "2SZKHeuUwBEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "  def __init__(self, tokenizer, file_path:str, max_length:int):\n",
        "    pad_token_idx = tokenizer.pad_token_id\n",
        "    self.docs = []\n",
        "    # for line in csv_datas: # line[0] 한글, line[1] 영어\n",
        "    for i in range(len(num_data)):\n",
        "      input = tokenizer.encode(num_data['origin_lang'][i],max_length=max_length,truncation=True)\n",
        "      rest = max_length - len(input)\n",
        "      input = torch.tensor(input + [pad_token_idx]*rest)\n",
        "\n",
        "      target = tokenizer.encode(num_data['pronun_list'][i], max_length=max_length, truncation=True)\n",
        "      rest = max_length - len(target)\n",
        "      target = torch.tensor(target+ [pad_token_idx] * rest)\n",
        "\n",
        "      doc={\n",
        "        'input_str': tokenizer.convert_ids_to_tokens(input),\n",
        "        'input':input,                                        # input\n",
        "        'input_mask': (input != pad_token_idx).unsqueeze(-2),       # input_mask\n",
        "        'target_str': tokenizer.convert_ids_to_tokens(target),\n",
        "        'target': target,                                       # target,\n",
        "        'target_mask': self.make_std_mask(target, pad_token_idx),    # target_mask\n",
        "        'token_num': (target[...,1:] != pad_token_idx).data.sum()  # token_num\n",
        "      }\n",
        "      self.docs.append(doc)\n",
        "  @staticmethod\n",
        "  def make_std_mask(tgt, pad_token_idx):\n",
        "    'Create a mask to hide padding and future words.'\n",
        "    target_mask = (tgt != pad_token_idx).unsqueeze(-2)\n",
        "    target_mask = target_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(target_mask.data))\n",
        "    return target_mask.squeeze()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.docs)\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.docs[idx]\n",
        "    return item"
      ],
      "metadata": {
        "id": "43yiR4XgzF7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from KoCharELECTRA.tokenization_kocharelectra import KoCharElectraTokenizer\n",
        "\n",
        "class TranslationTrainer():\n",
        "  def __init__(self,\n",
        "               dataset,\n",
        "               tokenizer,\n",
        "               model,\n",
        "               max_len,\n",
        "               device,\n",
        "               model_name,\n",
        "               checkpoint_path,\n",
        "               batch_size,\n",
        "               ):\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model = model\n",
        "    self.max_len = max_len\n",
        "    self.model_name = model_name\n",
        "    self.checkpoint_path = checkpoint_path\n",
        "    self.device = device\n",
        "    self.ntoken = tokenizer.vocab_size\n",
        "    self.batch_size = batch_size\n",
        "  def my_collate_fn(self, samples):\n",
        "    input_str =[]\n",
        "    target_str =[]\n",
        "    input_str.append([sample['input_str'] for sample in samples])\n",
        "    input = [sample['input'] for sample in samples]\n",
        "    input_mask = [sample['input_mask'] for sample in samples]\n",
        "    target = [sample['target'] for sample in samples]\n",
        "    target_mask = [sample['target_mask'] for sample in samples]\n",
        "    token_num = [sample['token_num'] for sample in samples]\n",
        "    target_str.append([sample['target_str'] for sample in samples])\n",
        "\n",
        "    return {\n",
        "        \"input_str\":input_str,\n",
        "        \"input\":torch.stack(input).contiguous(),                                              # input\n",
        "        \"input_mask\": torch.stack(input_mask).contiguous(),       # input_mask\n",
        "        \"target\": torch.stack(target).contiguous(),                                           # target,\n",
        "        \"target_mask\": torch.stack(target_mask).contiguous(),   # target_mask\n",
        "        \"token_num\": torch.stack(token_num).contiguous(),   # token_num\n",
        "        \"target_str\": target_str\n",
        "    }\n",
        "  def build_dataloaders(self, train_test_split=0.1, train_shuffle=True, eval_shuffle=True):\n",
        "    dataset_len = len(self.dataset)\n",
        "    eval_len = int(dataset_len * train_test_split)\n",
        "    train_len = dataset_len - eval_len\n",
        "    train_dataset, eval_dataset = random_split(self.dataset, (train_len, eval_len))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=train_shuffle, collate_fn=self.my_collate_fn)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=self.batch_size, shuffle=eval_shuffle , collate_fn=self.my_collate_fn)\n",
        "\n",
        "    return train_loader, eval_loader\n",
        "\n",
        "  def train(self, epochs, train_dataset, eval_dataset, optimizer, scheduler):\n",
        "    self.model.train()\n",
        "    total_loss = 0.\n",
        "    global_steps = 0\n",
        "    start_time = time.time()\n",
        "    losses = {}\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model = None\n",
        "    start_epoch = 0\n",
        "    start_step = 0\n",
        "    train_dataset_length = len(train_dataset)\n",
        "\n",
        "    self.model.to(self.device)\n",
        "    if os.path.isfile(f'{self.checkpoint_path}/{self.model_name}.pth'):\n",
        "      checkpoint = torch.load(f'{self.checkpoint_path}/{self.model_name}.pth', map_location=self.device)\n",
        "      start_epoch = checkpoint['epoch']\n",
        "      losses = checkpoint['losses']\n",
        "      global_steps = checkpoint['train_step']\n",
        "      start_step = global_steps if start_epoch == 0 else (global_steps % train_dataset_length) + 1\n",
        "\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "      epoch_start_time = time.time()\n",
        "\n",
        "      pb = tqdm(enumerate(train_dataset),\n",
        "                desc=f'Epoch-{epoch} Iterator',\n",
        "                total=train_dataset_length,\n",
        "                bar_format='{l_bar}{bar:10}{r_bar}'\n",
        "                )\n",
        "      pb.update(start_step)\n",
        "      for i,data in pb:\n",
        "        if i < start_step:\n",
        "          continue\n",
        "        \"\"\"\n",
        "        doc={\n",
        "        \"input\":input,                                              # input\n",
        "        \"input_mask\": (input != pad_token_idx).unsqueeze(-2),       # input_mask\n",
        "        \"target\": target,                                           # target,\n",
        "        \"target_mask\": self.make_std_mask(target, pad_token_idx),   # target_mask\n",
        "        \"token_num\": (target[...,1:] != pad_token_idx).data.sum()   # token_num\n",
        "      }\n",
        "        \"\"\"\n",
        "        input = data['input'].to(self.device)\n",
        "        target = data['target'].to(self.device)\n",
        "        input_mask = data['input_mask'].to(self.device)\n",
        "        target_mask = data['target_mask'].to(self.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        generator_logit, loss = self.model.forward(input, target, input_mask, target_mask, labels=target)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses[global_steps] = loss.item()\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 1\n",
        "        save_interval = 500\n",
        "\n",
        "        global_steps += 1\n",
        "\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "          cur_loss = total_loss / log_interval\n",
        "          elapsed = time.time() - start_time\n",
        "          # print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "          #       'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "          #       'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "          #         epoch, i, len(train_dataset), scheduler.get_lr()[0],\n",
        "          #         elapsed * 1000 / log_interval,\n",
        "          #         cur_loss, math.exp(cur_loss)))\n",
        "          pb.set_postfix_str('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                             'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                             'loss {:5.2f}'.format(\n",
        "            epoch, i, len(train_dataset), scheduler.get_lr()[0],\n",
        "            elapsed * 1000 / log_interval,\n",
        "            cur_loss))\n",
        "          total_loss = 0\n",
        "          start_time = time.time()\n",
        "          # self.save(epoch, self.model, optimizer, losses, global_steps)\n",
        "          if i % save_interval == 0:\n",
        "            self.save(epoch, self.model, optimizer, losses, global_steps)\n",
        "      val_loss, val_acc = self.evaluate(eval_dataset)\n",
        "      self.model.train()\n",
        "      print('-' * 89)\n",
        "      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid acc {:5.2f}'\n",
        "            .format(epoch, (time.time() - epoch_start_time),\n",
        "                                       val_loss, val_acc))\n",
        "      print('-' * 89)\n",
        "      if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "      start_step = 0\n",
        "      scheduler.step()\n",
        "\n",
        "  def evaluate(self, dataset):\n",
        "    self.model.eval()  # 평가 모드를 시작합니다.\n",
        "    total_loss = 0.\n",
        "    total_correct = 0  # 추가: 전체 맞은 갯수를 저장하기 위한 변수\n",
        "    total_tokens = 0  # 추가: 전체 토큰의 갯수를 저장하기 위한 변수\n",
        "\n",
        "    self.model.to(self.device)\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(dataset):\n",
        "        input = data['input'].to(self.device)\n",
        "        target = data['target'].to(self.device)\n",
        "        input_mask = data['input_mask'].to(self.device)\n",
        "        target_mask = data['target_mask'].to(self.device)\n",
        "\n",
        "        generator_logit, loss = self.model.forward(input, target, input_mask, target_mask, labels=target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 정확도 계산 추가\n",
        "        pred = torch.max(generator_logit, dim=2)[1]  # 가장 높은 로짓 값을 가진 인덱스를 가져옴\n",
        "        total_correct += (pred == target).sum().item()  # 맞은 갯수를 누적\n",
        "        total_tokens += target.numel()  # 토큰의 총 갯수를 누적\n",
        "\n",
        "    accuracy = total_correct / total_tokens  # 정확도 계산\n",
        "    return total_loss / (len(dataset) - 1), accuracy\n",
        "\n",
        "  def save(self, epoch, model, optimizer, losses, train_step):\n",
        "    torch.save({\n",
        "      'epoch': epoch,  # 현재 학습 epoch\n",
        "      'model_state_dict': model.state_dict(),  # 모델 저장\n",
        "      'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n",
        "      'losses': losses,  # Loss 저장\n",
        "      'train_step': train_step,  # 현재 진행한 학습\n",
        "    }, f'{self.checkpoint_path}/{self.model_name}.pth')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  torch.manual_seed(42)\n",
        "  # model setting\n",
        "  model_name = 'transformer-translation'\n",
        "  checkpoint_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "  max_length = 64\n",
        "  d_model = 512\n",
        "  head_num = 8\n",
        "  dropout = 0.1\n",
        "  N = 6\n",
        "  device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  tokenizer = KoCharElectraTokenizer.from_pretrained(\"monologg/kocharelectra-base-discriminator\")\n",
        "  vocab_num = len(tokenizer.get_vocab())\n",
        "  # hyper parameter\n",
        "  epochs = 50\n",
        "  batch_size = 64\n",
        "  padding_idx = tokenizer.pad_token_id\n",
        "  learning_rate = 0.5\n",
        "\n",
        "  dataset = TranslationDataset(tokenizer=tokenizer, file_path=num_data, max_length=max_length)\n",
        "  model = Transformer(vocab_num=vocab_num,\n",
        "                      d_model=d_model,\n",
        "                      max_seq_len=max_length,\n",
        "                      head_num=head_num,\n",
        "                      dropout=dropout,\n",
        "                      N=N)\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "  trainer = TranslationTrainer(dataset, tokenizer, model, max_length, device, model_name, checkpoint_path, batch_size)\n",
        "  train_dataloader, eval_dataloader = trainer.build_dataloaders(train_test_split=0.2)\n",
        "\n",
        "  trainer.train(epochs, train_dataloader, eval_dataloader, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "8c230c1045ed4e5f9236e53c5cbdf3ce",
            "b37a5e0a47944df8a45d01860c1cc571",
            "d25200e0c8e44cc48f69ed6206d9016c",
            "39f8d2300fd246ed8a1461065a7b5b09",
            "07b8f9da03194cdfac621ae9ae859ab1",
            "b5ce4c49f9804e2daee824fbbfd3270c",
            "5902fea5ff0f46eda2fa25adbe00a731",
            "6db5a307a42941e889d0aac9d09a662a",
            "eb88c2acaa3246bf82896d54e515ada0",
            "7525c7ae5d794239a1670bcb68a19a7d",
            "0470b11d84d6456c8290714f394439b1",
            "a422c17bbca34aa5a288713b7cc3fc3b",
            "0fa6af83a77f4f1b9505d8363bcc2bf8",
            "f0aa164b04d5407bbe3209e1670071f8",
            "3275f2cf6c2042bc90d1ad072eca285c",
            "f8b3d9f7a0da41d8b09e50a2bca53861",
            "ea70492c835243da9775d4953407a336",
            "0c675add6c384bfd8cc585f34b0970e7",
            "ae47e30e807b4397b6ba58c3e4db5f1b",
            "f96f1c560e8640c588beb355d524d358",
            "0b9841198cda400099895688df597823",
            "7ecc04878e0640d09f436652dd1794c7",
            "99a53822b10841a2b585204ccf5bd1c4",
            "c8f56958d3414de2b20fec51c8ac1d22",
            "b6628775b3854f509c9479debf0f70b5",
            "2929cb1d5c53445a96bca572259fcbef",
            "ab72d0ce54d541f7a2bdf9d11a97f9bc",
            "839ca4e82cce4a9f83f892c47e4271f3",
            "39a90263d94542cf8379806852e3a4b8",
            "eb4b4fbe732c473d8a4333c7f6eb52c9",
            "53daf758f5da42899a3c765ab665d6f4",
            "9a6f08e6550c4cdc9641f2170e524b26",
            "c58f35170f444dd8aaf284f793bc9951"
          ]
        },
        "id": "Ws5RsVoIyi9h",
        "outputId": "6c23c8e1-a33e-451f-b6af-f92b5e40d05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/59.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c230c1045ed4e5f9236e53c5cbdf3ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a422c17bbca34aa5a288713b7cc3fc3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/487 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a53822b10841a2b585204ccf5bd1c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
            "The class this function is called from is 'KoCharElectraTokenizer'.\n",
            "Epoch-0 Iterator: 100%|██████████| 766/766 [07:06<00:00,  1.80it/s, | epoch   0 |   765/  766 batches | lr 0.50 | ms/batch 480.60 | loss  8.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | time: 464.23s | valid loss  5.75 | valid acc  0.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch-1 Iterator: 100%|██████████| 766/766 [07:11<00:00,  1.78it/s, | epoch   1 |   765/  766 batches | lr 0.45 | ms/batch 478.54 | loss  5.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 469.07s | valid loss  4.74 | valid acc  0.01\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch-2 Iterator: 100%|██████████| 766/766 [07:14<00:00,  1.76it/s, | epoch   2 |   765/  766 batches | lr 0.43 | ms/batch 477.54 | loss  4.60]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 472.30s | valid loss  4.09 | valid acc  0.01\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch-3 Iterator: 100%|██████████| 766/766 [07:11<00:00,  1.78it/s, | epoch   3 |   765/  766 batches | lr 0.41 | ms/batch 483.93 | loss  3.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 469.16s | valid loss  3.83 | valid acc  0.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch-4 Iterator:  85%|████████▍ | 649/766 [06:04<01:05,  1.78it/s, | epoch   4 |   648/  766 batches | lr 0.39 | ms/batch 562.17 | loss  3.94]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q471tleM7eWC",
        "outputId": "0b514e52-a264-4ee5-bfe5-773cdfa8e04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoCharELECTRA'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 37 (delta 16), reused 22 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (37/37), 58.87 KiB | 247.00 KiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZs2cyWV2jlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}